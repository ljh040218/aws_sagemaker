# 1. PerformanceMetricsCallback í´ë˜ìŠ¤ ìˆ˜ì • - ìˆ˜ë ´ ê²€ì‚¬ ê¸°ëŠ¥ ì¶”ê°€

class PerformanceMetricsCallback(BaseCallback):
    """ì‹¤ì‹œê°„ ì„±ëŠ¥ ì¶”ì  ë° ì¡°ê¸° ì¢…ë£Œ"""
    
    def __init__(self, eval_env, eval_freq=2000, verbose=0, 
                 convergence_threshold=0.9, patience=10, min_episodes=50):
        super(PerformanceMetricsCallback, self).__init__(verbose)
        self.eval_env = eval_env
        self.eval_freq = eval_freq
        
        # ì¡°ê¸° ì¢…ë£Œ ê´€ë ¨ íŒŒë¼ë¯¸í„°
        self.convergence_threshold = convergence_threshold  # 90% ìˆ˜ë ´ ê¸°ì¤€
        self.patience = patience  # ì—°ì†ìœ¼ë¡œ ê°œì„ ë˜ì§€ ì•ŠëŠ” ì—í”¼ì†Œë“œ ìˆ˜
        self.min_episodes = min_episodes  # ìµœì†Œ í•™ìŠµ ì—í”¼ì†Œë“œ
        
        # ìˆ˜ë ´ ì¶”ì  ë³€ìˆ˜
        self.best_performance = -np.inf
        self.performance_history = []
        self.episodes_without_improvement = 0
        self.converged = False
        
        self.metrics_history = {
            'episode': [],
            'timestep': [],
            'energy_efficiency': [],
            'soc_decrease_rate': [],
            'speed_tracking_rate': [],
            'target_speed_proximity': [],
            'comfort_score': [],
            'safety_violations': [],
            'convergence_reward': [],
            'learning_stability': [],
            'performance_score': []  # ì¢…í•© ì„±ëŠ¥ ì ìˆ˜
        }
        self.episode_rewards = []
        self.last_episode_count = 0
    
    def _calculate_performance_score(self, metrics):
        """ì¢…í•© ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚° (0~1 ë²”ìœ„)"""
        # ì—ë„ˆì§€ íš¨ìœ¨ì„± (40% ê°€ì¤‘ì¹˜)
        efficiency_score = min(metrics.get('energy_efficiency', 1.0) / 10.0, 1.0) * 0.4
        
        # ì†ë„ ì¶”ì¢…ë¥  (30% ê°€ì¤‘ì¹˜)  
        speed_score = (metrics.get('speed_tracking_rate', 0) / 100.0) * 0.3
        
        # SOC ê´€ë¦¬ (20% ê°€ì¤‘ì¹˜) - ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
        soc_rate = metrics.get('soc_decrease_rate', 100)
        soc_score = max(0, (100 - soc_rate) / 100.0) * 0.2
        
        # ì•ˆì „ì„± (10% ê°€ì¤‘ì¹˜)
        safety_violations = metrics.get('safety_violations', 10)
        safety_score = max(0, (10 - safety_violations) / 10.0) * 0.1
        
        total_score = efficiency_score + speed_score + soc_score + safety_score
        return min(1.0, max(0.0, total_score))
    
    def _check_convergence(self):
        """ìˆ˜ë ´ ì¡°ê±´ ê²€ì‚¬"""
        if len(self.performance_history) < self.min_episodes:
            return False
        
        # ìµœê·¼ ì„±ëŠ¥ì˜ ì•ˆì •ì„± ê²€ì‚¬
        recent_scores = self.performance_history[-10:]  # ìµœê·¼ 10ê°œ í‰ê°€
        if len(recent_scores) < 10:
            return False
        
        # í‘œì¤€í¸ì°¨ê°€ ì‘ê³  í‰ê·  ì„±ëŠ¥ì´ ë†’ìœ¼ë©´ ìˆ˜ë ´ìœ¼ë¡œ íŒë‹¨
        mean_performance = np.mean(recent_scores)
        std_performance = np.std(recent_scores)
        
        # ìˆ˜ë ´ ì¡°ê±´:
        # 1. í‰ê·  ì„±ëŠ¥ì´ threshold ì´ìƒ
        # 2. ì„±ëŠ¥ì˜ ë³€ë™ì„±ì´ ì‘ìŒ (std < 0.05)
        # 3. ìµœê·¼ ì„±ëŠ¥ ê°œì„ ì´ ì—†ìŒ
        performance_stable = std_performance < 0.05
        performance_high = mean_performance >= self.convergence_threshold
        
        if performance_stable and performance_high:
            logger.info(f"ìˆ˜ë ´ ê°ì§€: í‰ê· ì„±ëŠ¥={mean_performance:.3f}, ì•ˆì •ì„±={std_performance:.4f}")
            return True
        
        return False
    
    def _on_step(self) -> bool:
        if self.n_calls % self.eval_freq == 0:
            try:
                # í™˜ê²½ì—ì„œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                if hasattr(self.training_env, 'get_attr'):
                    current_metrics_list = self.training_env.get_attr('get_current_metrics')
                    episode_counts = self.training_env.get_attr('episode_count')
                    
                    if current_metrics_list and episode_counts:
                        current_metrics = current_metrics_list[0]() if callable(current_metrics_list[0]) else current_metrics_list[0]
                        current_episode = episode_counts[0]
                    else:
                        current_metrics = {'energy_efficiency': 4.0}
                        current_episode = 0
                else:
                    if hasattr(self.training_env, 'get_current_metrics'):
                        current_metrics = self.training_env.get_current_metrics()
                    else:
                        current_metrics = {'energy_efficiency': 4.0}
                    
                    if hasattr(self.training_env, 'episode_count'):
                        current_episode = self.training_env.episode_count
                    else:
                        current_episode = 0
                
                # ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚°
                performance_score = self._calculate_performance_score(current_metrics)
                self.performance_history.append(performance_score)
                
                # íš¨ìœ¨ê°’ ë¡œê·¸ ì¶œë ¥
                efficiency = current_metrics.get('energy_efficiency', 4.0)
                logger.info(f"Step {self.n_calls}: ì„±ëŠ¥ì ìˆ˜={performance_score:.3f}, íš¨ìœ¨={efficiency:.3f} km/kWh")
                
                # ìƒˆ ì—í”¼ì†Œë“œ ì™„ë£Œì‹œë§Œ ê¸°ë¡
                if current_episode > self.last_episode_count:
                    self.last_episode_count = current_episode
                    
                    # ì„±ëŠ¥ ê°œì„  ì¶”ì 
                    if performance_score > self.best_performance:
                        self.best_performance = performance_score
                        self.episodes_without_improvement = 0
                        logger.info(f" ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥: {performance_score:.3f}")
                    else:
                        self.episodes_without_improvement += 1
                    
                    # ë©”íŠ¸ë¦­ ê¸°ë¡
                    self.metrics_history['timestep'].append(self.n_calls)
                    self.metrics_history['episode'].append(current_episode)
                    self.metrics_history['energy_efficiency'].append(efficiency)
                    self.metrics_history['performance_score'].append(performance_score)
                    self.metrics_history['soc_decrease_rate'].append(
                        current_metrics.get('soc_decrease_rate', 15.0)
                    )
                    self.metrics_history['speed_tracking_rate'].append(
                        current_metrics.get('speed_tracking_rate', 85.0)
                    )
                    
                    # ìˆ˜ë ´ ê²€ì‚¬
                    if self._check_convergence():
                        logger.info(" 90% ìˆ˜ë ´ ë‹¬ì„±! í•™ìŠµì„ ì¡°ê¸° ì¢…ë£Œí•©ë‹ˆë‹¤.")
                        self.converged = True
                        return False  # í•™ìŠµ ì¤‘ë‹¨
                    
                    # Patience ì²´í¬
                    if self.episodes_without_improvement >= self.patience:
                        logger.info(f" {self.patience}íšŒ ì—°ì† ê°œì„  ì—†ìŒ. ì¡°ê¸° ì¢…ë£Œ ê²€í† ...")
                        if len(self.performance_history) >= self.min_episodes:
                            recent_performance = np.mean(self.performance_history[-5:])
                            if recent_performance >= 0.8:  # 80% ì´ìƒì´ë©´ ì¢…ë£Œ
                                logger.info(" ì¶©ë¶„í•œ ì„±ëŠ¥ ë‹¬ì„±ìœ¼ë¡œ ì¡°ê¸° ì¢…ë£Œí•©ë‹ˆë‹¤.")
                                return False
                        
            except Exception as e:
                logger.warning(f"Metrics collection failed at step {self.n_calls}: {e}")
                
        return True


# 2. train_sac_model í•¨ìˆ˜ ìˆ˜ì • - ì¡°ê¸° ì¢…ë£Œ ì½œë°± ì ìš©

def train_sac_model(model_name, is_transfer_learning=False, total_timesteps=100000, 
                   data_dir="./data", save_dir="./models", 
                   enable_early_stopping=True, convergence_threshold=0.9):
    """SAC ëª¨ë¸ í›ˆë ¨ - ì¡°ê¸° ì¢…ë£Œ ê¸°ëŠ¥ í¬í•¨"""
    
    logger.info(f" SAC ëª¨ë¸ í›ˆë ¨ ì‹œì‘: {model_name}")
    logger.info(f"ì „ì´í•™ìŠµ: {is_transfer_learning}")
    logger.info(f"ì´ ìŠ¤í…: {total_timesteps}")
    logger.info(f"ì¡°ê¸° ì¢…ë£Œ: {enable_early_stopping} (ì„ê³„ê°’: {convergence_threshold})")
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(save_dir, exist_ok=True)
    os.makedirs("./results", exist_ok=True)
    
    # í™˜ê²½ ìƒì„±
    env = EVEnergyEnvironmentPreprocessed(data_dir=data_dir)
    
    # SageMaker ìµœì í™”ëœ SAC ì„¤ì •
    sac_config = {
        'learning_rate': 3e-4,
        'buffer_size': 100000,
        'batch_size': 256,
        'tau': 0.005,
        'gamma': 0.99,
        'train_freq': 1,
        'gradient_steps': 1,
        'verbose': 1,
        'device': 'cuda' if hasattr(torch, 'cuda') and torch.cuda.is_available() else 'cpu'
    }
    
    # ëª¨ë¸ ìƒì„± (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
    if is_transfer_learning:
        logger.info("ì „ì´í•™ìŠµ ëª¨ë¸ ë¡œë“œ ì‹œë„...")
        
        if install_transfer_learning_dependencies():
            try:
                from huggingface_sb3 import load_from_hub
                
                pretrained_models = [
                    ("sb3/sac-LunarLanderContinuous-v2", "sac-LunarLanderContinuous-v2.zip"),
                    ("sb3/sac-BipedalWalker-v3", "sac-BipedalWalker-v3.zip"),
                    ("sb3/sac-Pendulum-v1", "sac-Pendulum-v1.zip")
                ]
                
                model = None
                for repo_id, filename in pretrained_models:
                    try:
                        logger.info(f"ğŸ” {repo_id} ëª¨ë¸ ë¡œë“œ ì‹œë„...")
                        checkpoint = load_from_hub(repo_id=repo_id, filename=filename)
                        temp_model = SAC.load(checkpoint)
                        
                        model = SAC(policy=temp_model.policy_class, env=env, **sac_config)
                        
                        try:
                            model.policy.load_state_dict(temp_model.policy.state_dict(), strict=False)
                            logger.info(f" {repo_id} ëª¨ë¸ì—ì„œ ì „ì´í•™ìŠµ ì„±ê³µ!")
                            break
                        except:
                            logger.warning(f" {repo_id} íŒŒë¼ë¯¸í„° ë³µì‚¬ ì‹¤íŒ¨, ë‹¤ìŒ ëª¨ë¸ ì‹œë„")
                            continue
                            
                    except Exception as e:
                        logger.warning(f" {repo_id} ë¡œë“œ ì‹¤íŒ¨: {e}")
                        continue
                
                if model is None:
                    logger.warning(" ì „ì´í•™ìŠµ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, ìˆœìˆ˜ í•™ìŠµìœ¼ë¡œ ì§„í–‰")
                    model = SAC('MlpPolicy', env, **sac_config)
                    
            except ImportError:
                logger.warning(" huggingface_sb3 ì„¤ì¹˜ ì‹¤íŒ¨, ìˆœìˆ˜ í•™ìŠµìœ¼ë¡œ ì§„í–‰")
                model = SAC('MlpPolicy', env, **sac_config)
            except Exception as e:
                logger.warning(f" ì „ì´í•™ìŠµ ì‹¤íŒ¨: {e}, ìˆœìˆ˜ í•™ìŠµìœ¼ë¡œ ì§„í–‰")
                model = SAC('MlpPolicy', env, **sac_config)
        else:
            logger.warning(" ì „ì´í•™ìŠµ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì‹¤íŒ¨, ìˆœìˆ˜ í•™ìŠµìœ¼ë¡œ ì§„í–‰")
            model = SAC('MlpPolicy', env, **sac_config)
    else:
        logger.info("ìˆœìˆ˜ í•™ìŠµ ëª¨ë¸ ìƒì„±")
        model = SAC('MlpPolicy', env, **sac_config)
    
    # ì¡°ê¸° ì¢…ë£Œ ì½œë°± ì„¤ì •
    if enable_early_stopping:
        eval_callback = PerformanceMetricsCallback(
            eval_env=env,
            eval_freq=2000,
            convergence_threshold=convergence_threshold,
            patience=10,
            min_episodes=20,  # ìµœì†Œ 20 ì—í”¼ì†Œë“œëŠ” í•™ìŠµ
            verbose=1
        )
        logger.info(f" ì¡°ê¸° ì¢…ë£Œ í™œì„±í™”: ì„ê³„ê°’={convergence_threshold}, patience=10")
    else:
        eval_callback = PerformanceMetricsCallback(eval_env=env, eval_freq=2000, verbose=1)
        logger.info(" ê³ ì • timestepsë¡œ í•™ìŠµ")
    
    # í›ˆë ¨ ì‹œì‘
    logger.info(f" í•™ìŠµ ì‹œì‘ - ìµœëŒ€: {total_timesteps} ìŠ¤í…")
    start_time = datetime.now()
    
    actual_timesteps_trained = 0
    early_stopped = False
    
    try:
        model.learn(
            total_timesteps=total_timesteps,
            callback=eval_callback,
            log_interval=100,
            progress_bar=True
        )
        
        # ì‹¤ì œ í•™ìŠµëœ ìŠ¤í… ìˆ˜ í™•ì¸
        actual_timesteps_trained = model.num_timesteps
        early_stopped = getattr(eval_callback, 'converged', False)
        
        end_time = datetime.now()
        training_time = (end_time - start_time).total_seconds()
        
        if early_stopped:
            logger.info(f" ì¡°ê¸° ì¢…ë£Œë¡œ í•™ìŠµ ì™„ë£Œ! ì‹¤ì œ ìŠ¤í…: {actual_timesteps_trained}/{total_timesteps}")
            logger.info(f" ì†Œìš”ì‹œê°„: {training_time:.1f}ì´ˆ (ì˜ˆìƒ ì‹œê°„ ëŒ€ë¹„ ë‹¨ì¶•)")
        else:
            logger.info(f" ì „ì²´ í•™ìŠµ ì™„ë£Œ! ì†Œìš”ì‹œê°„: {training_time:.1f}ì´ˆ")
        
    except KeyboardInterrupt:
        logger.info(" ì‚¬ìš©ì ì¤‘ë‹¨")
    except Exception as e:
        logger.error(f" í•™ìŠµ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return None, None
    
    # ëª¨ë¸ ì €ì¥
    model_path = f"{save_dir}/{model_name}.zip"
    model.save(model_path)
    logger.info(f" ëª¨ë¸ ì €ì¥: {model_path}")
    
    # ì„±ëŠ¥ í‰ê°€ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
    logger.info(" ìµœì¢… ì„±ëŠ¥ í‰ê°€ ì¤‘...")
    eval_results, eval_episodes = evaluate_policy(
        model, env, n_eval_episodes=50, return_episode_rewards=True
    )
    
    # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
    final_metrics = []
    for _ in range(50):
        obs, _ = env.reset()
        done = False
        while not done:
            action, _ = model.predict(obs, deterministic=True)
            obs, _, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
        final_metrics.append(env.get_episode_metrics())
    
    # ê²°ê³¼ ì •ë¦¬ - ì¡°ê¸° ì¢…ë£Œ ì •ë³´ ì¶”ê°€
    results = {
        'model_name': model_name,
        'is_transfer_learning': is_transfer_learning,
        'training_time': training_time,
        'total_timesteps': total_timesteps,
        'actual_timesteps': actual_timesteps_trained,  # ì‹¤ì œ í•™ìŠµ ìŠ¤í…
        'early_stopped': early_stopped,  # ì¡°ê¸° ì¢…ë£Œ ì—¬ë¶€
        'convergence_threshold': convergence_threshold,
        'time_efficiency': (total_timesteps - actual_timesteps_trained) / total_timesteps if early_stopped else 0,
        'eval_mean_reward': np.mean(eval_results),
        'eval_std_reward': np.std(eval_results),
        'metrics': {
            'energy_efficiency': {
                'mean': np.mean([m['energy_efficiency'] for m in final_metrics]),
                'std': np.std([m['energy_efficiency'] for m in final_metrics]),
                'values': [m['energy_efficiency'] for m in final_metrics]
            },
            'speed_tracking_rate': {
                'mean': np.mean([m['speed_tracking_rate'] for m in final_metrics]),
                'std': np.std([m['speed_tracking_rate'] for m in final_metrics]),
                'values': [m['speed_tracking_rate'] for m in final_metrics]
            },
            'soc_decrease_rate': {
                'mean': np.mean([m['soc_decrease_rate'] for m in final_metrics]),
                'std': np.std([m['soc_decrease_rate'] for m in final_metrics]),
                'values': [m['soc_decrease_rate'] for m in final_metrics]
            }
        },
        'learning_history': eval_callback.metrics_history,
        'convergence_info': {
            'performance_history': getattr(eval_callback, 'performance_history', []),
            'best_performance': getattr(eval_callback, 'best_performance', 0),
            'episodes_without_improvement': getattr(eval_callback, 'episodes_without_improvement', 0)
        }
    }
    
    # ê²°ê³¼ ì €ì¥ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼í•˜ê²Œ JSON ì§ë ¬í™”)
    results_path = f"./results/{model_name}_results.json"
    with open(results_path, 'w') as f:
        def serialize_for_json(obj):
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, dict):
                return {k: serialize_for_json(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [serialize_for_json(item) for item in obj]
            else:
                return obj
        
        json.dump(serialize_for_json(results), f, indent=2)
    
    logger.info(f" ê²°ê³¼ ì €ì¥: {results_path}")
    
    if early_stopped:
        time_saved = (total_timesteps - actual_timesteps_trained) / total_timesteps * 100
        logger.info(f" ì¡°ê¸° ì¢…ë£Œë¡œ {time_saved:.1f}% ì‹œê°„ ì ˆì•½!")
    
    return model, results


# 3. main í•¨ìˆ˜ì— ì¡°ê¸° ì¢…ë£Œ ì˜µì…˜ ì¶”ê°€

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - ì¡°ê¸° ì¢…ë£Œ ê¸°ëŠ¥ í¬í•¨"""
    parser = argparse.ArgumentParser(description='SAC ì „ê¸°ì°¨ ì—ë„ˆì§€ íš¨ìœ¨ ìµœì í™” - ì¡°ê¸° ì¢…ë£Œ ê¸°ëŠ¥')
    parser.add_argument('--data_dir', type=str, default='./data', 
                       help='ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ')
    parser.add_argument('--timesteps_scratch', type=int, default=100000,
                       help='ìˆœìˆ˜í•™ìŠµ ìµœëŒ€ ìŠ¤í… ìˆ˜')
    parser.add_argument('--timesteps_transfer', type=int, default=50000,
                       help='ì „ì´í•™ìŠµ ìµœëŒ€ ìŠ¤í… ìˆ˜')
    parser.add_argument('--mode', type=str, choices=['compare', 'train_scratch', 'train_transfer'], 
                       default='compare', help='ì‹¤í–‰ ëª¨ë“œ')
    parser.add_argument('--aws_instance', type=str, default='ml.m5.xlarge',
                       help='AWS SageMaker ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…')
    
    # ì¡°ê¸° ì¢…ë£Œ ê´€ë ¨ ì˜µì…˜ ì¶”ê°€
    parser.add_argument('--enable_early_stopping', action='store_true', default=True,
                       help='ì¡°ê¸° ì¢…ë£Œ í™œì„±í™” (ê¸°ë³¸: True)')
    parser.add_argument('--convergence_threshold', type=float, default=0.9,
                       help='ìˆ˜ë ´ ì„ê³„ê°’ (ê¸°ë³¸: 0.9 = 90%)')
    parser.add_argument('--patience', type=int, default=10,
                       help='ê°œì„  ì—†ì´ ê¸°ë‹¤ë¦´ ì—í”¼ì†Œë“œ ìˆ˜ (ê¸°ë³¸: 10)')
    
    args = parser.parse_args()
    
    # SageMaker í™˜ê²½ ì •ë³´ ë¡œê¹…
    logger.info(" SageMaker ìµœì í™” SAC ì‹¤í—˜ ì‹œì‘ (ì¡°ê¸° ì¢…ë£Œ ê¸°ëŠ¥ í¬í•¨)")
    logger.info(f"ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…: {args.aws_instance}")
    logger.info(f"ë°ì´í„° ë””ë ‰í† ë¦¬: {args.data_dir}")
    logger.info(f"ì‹¤í–‰ ëª¨ë“œ: {args.mode}")
    logger.info(f"ì¡°ê¸° ì¢…ë£Œ: {args.enable_early_stopping} (ì„ê³„ê°’: {args.convergence_threshold})")
    
    if args.mode == 'compare':
        # compare_models_and_baseline í•¨ìˆ˜ë„ ìˆ˜ì •í•´ì•¼ í•¨ (train_sac_model í˜¸ì¶œ ë¶€ë¶„)
        results = compare_models_and_baseline_with_early_stopping(
            enable_early_stopping=args.enable_early_stopping,
            convergence_threshold=args.convergence_threshold
        )
        
    elif args.mode == 'train_scratch':
        model, results = train_sac_model(
            model_name="sac_from_scratch",
            is_transfer_learning=False,
            total_timesteps=args.timesteps_scratch,
            data_dir=args.data_dir,
            enable_early_stopping=args.enable_early_stopping,
            convergence_threshold=args.convergence_threshold
        )
        
    elif args.mode == 'train_transfer':
        model, results = train_sac_model(
            model_name="sac_with_transfer", 
            is_transfer_learning=True,
            total_timesteps=args.timesteps_transfer,
            data_dir=args.data_dir,
            enable_early_stopping=args.enable_early_stopping,
            convergence_threshold=args.convergence_threshold
        )
    
    logger.info(" SageMaker ì‹¤í—˜ ì™„ë£Œ")


# 4. compare_models_and_baseline í•¨ìˆ˜ë„ ìˆ˜ì • í•„ìš”

def compare_models_and_baseline_with_early_stopping(enable_early_stopping=True, convergence_threshold=0.9):
    """ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë° ë¶„ì„ - ì¡°ê¸° ì¢…ë£Œ ê¸°ëŠ¥ í¬í•¨"""
    
    logger.info("=" * 60)
    logger.info("SAC ì „ê¸°ì°¨ ì—ë„ˆì§€ íš¨ìœ¨ ìµœì í™” ì‹¤í—˜ ì‹œì‘ (ì¡°ê¸° ì¢…ë£Œ ê¸°ëŠ¥ í¬í•¨)")
    logger.info("=" * 60)
    
    # ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼í•œ ê¸°ì¤€ì„  í‰ê°€...
    
    # SAC ëª¨ë¸ í›ˆë ¨ ë¶€ë¶„ë§Œ ìˆ˜ì •
    sac_scratch_model, sac_scratch_results = train_sac_model(
        model_name="sac_from_scratch",
        is_transfer_learning=False,
        total_timesteps=100000,
        data_dir="./data",
        enable_early_stopping=enable_early_stopping,
        convergence_threshold=convergence_threshold
    )
    
    sac_transfer_model, sac_transfer_results = train_sac_model(
        model_name="sac_with_transfer",
        is_transfer_learning=True,
        total_timesteps=50000,
        data_dir="./data",
        enable_early_stopping=enable_early_stopping,
        convergence_threshold=convergence_threshold
    )
    
    # ë‚˜ë¨¸ì§€ëŠ” ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼...
    
    return comparison_results