# í™•ì¥ëœ ì „ì´í•™ìŠµ ë¹„êµ ì‹¤í—˜: MountainCar + LunarLander + ìˆœìˆ˜í•™ìŠµ
# í˜„ì¬ ì‹¤í—˜ ì™„ë£Œ í›„ ì‹¤í–‰í•  ì¶”ê°€ ë¹„êµ ì½”ë“œ

import pandas as pd
import numpy as np
from scipy import stats
import json
from datetime import datetime
import logging
import warnings
warnings.filterwarnings('ignore')

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ExtendedTransferLearningComparison:
    """í™•ì¥ëœ ì „ì´í•™ìŠµ ë¹„êµ (4ê°œ ëª¨ë¸ ë™ì‹œ ë¹„êµ)"""
    
    def __init__(self):
        self.experiment_id = datetime.now().strftime('%Y%m%d_%H%M%S')
        self.results = {}
        
        # ë¹„êµí•  ëª¨ë¸ë“¤
        self.models_to_compare = {
            'cruise_mode': {'type': 'baseline', 'description': 'í¬ë£¨ì¦ˆ ëª¨ë“œ (PID ì œì–´)'},
            'sac_scratch': {'type': 'scratch', 'description': 'SAC ìˆœìˆ˜í•™ìŠµ (100k ìŠ¤í…)'},
            'sac_lunarlander': {'type': 'transfer', 'repo': 'sb3/sac-LunarLanderContinuous-v2', 'steps': 50000},
            'sac_mountaincar': {'type': 'transfer', 'repo': 'sb3/sac-MountainCarContinuous-v0', 'steps': 50000}
        }
        
        logger.info(" í™•ì¥ëœ ì „ì´í•™ìŠµ ë¹„êµ ì‹¤í—˜ ì´ˆê¸°í™”")
        logger.info("ë¹„êµ ëª¨ë¸: í¬ë£¨ì¦ˆ, ìˆœìˆ˜í•™ìŠµ, LunarLander ì „ì´, MountainCar ì „ì´")
    
    def load_previous_results(self):
        """ì´ì „ ì‹¤í—˜ ê²°ê³¼ ë¡œë“œ"""
        
        logger.info("ğŸ“‚ ì´ì „ ì‹¤í—˜ ê²°ê³¼ ë¡œë“œ ì¤‘...")
        
        try:
            # í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ì‹¤í—˜ ê²°ê³¼ ì°¾ê¸°
            import glob
            result_files = glob.glob('statistical_experiment_results_*.json')
            
            if result_files:
                latest_file = max(result_files)
                with open(latest_file, 'r') as f:
                    previous_results = json.load(f)
                
                logger.info(f" ì´ì „ ê²°ê³¼ ë¡œë“œ: {latest_file}")
                
                # ê¸°ì¡´ ê²°ê³¼ ì €ì¥
                if 'performance' in previous_results:
                    self.results['cruise_mode'] = previous_results['performance'].get('cruise_mode')
                    self.results['sac_scratch'] = previous_results['performance'].get('sac_scratch')
                    self.results['sac_lunarlander'] = previous_results['performance'].get('sac_lunarlander_transfer')
                
                return True
            else:
                logger.warning(" ì´ì „ ì‹¤í—˜ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return False
                
        except Exception as e:
            logger.error(f"ì´ì „ ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def create_ev_environment(self):
        """ì „ê¸°ì°¨ í™˜ê²½ ìƒì„± (ì¦ê°•ëœ ë°ì´í„° ì‚¬ìš©)"""
        
        try:
            from sagemaker_training import EVEnergyEnvironmentPreprocessed
            env = EVEnergyEnvironmentPreprocessed(data_dir="./")
            logger.info(" ì „ê¸°ì°¨ í™˜ê²½ ìƒì„± ì™„ë£Œ")
            return env
        except Exception as e:
            logger.error(f"í™˜ê²½ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def train_mountaincar_transfer_model(self, env):
        """MountainCar ì „ì´í•™ìŠµ ëª¨ë¸ í›ˆë ¨"""
        
        logger.info("ğŸ”ï¸ MountainCar ì „ì´í•™ìŠµ ì‹œì‘...")
        
        try:
            from huggingface_sb3 import load_from_hub
            from stable_baselines3 import SAC
            import torch
            
            # 1. MountainCar ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
            logger.info("ğŸ“¥ MountainCar ëª¨ë¸ ë‹¤ìš´ë¡œë“œ...")
            checkpoint = load_from_hub(
                repo_id="sb3/sac-MountainCarContinuous-v0",
                filename="sac-MountainCarContinuous-v0.zip"
            )
            
            # 2. ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë¡œë“œ ë° ë¶„ì„
            pretrained_model = SAC.load(checkpoint)
            logger.info(f" MountainCar ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            logger.info(f"   ê´€ì¸¡ ê³µê°„: {pretrained_model.observation_space.shape[0]}ì°¨ì›")
            logger.info(f"   í–‰ë™ ê³µê°„: {pretrained_model.action_space.shape[0]}ì°¨ì›")
            
            # 3. ì „ê¸°ì°¨ í™˜ê²½ìš© ìƒˆ ëª¨ë¸ ìƒì„±
            transfer_config = {
                'learning_rate': 1e-4,  # ë‚®ì€ í•™ìŠµë¥  (ë¯¸ì„¸ì¡°ì •)
                'buffer_size': 50000,
                'batch_size': 64,
                'tau': 0.01,
                'gamma': 0.95,
                'policy_kwargs': {
                    'net_arch': [128, 128],
                    'activation_fn': torch.nn.ReLU
                    # dropout ì œê±°ë¨
                },
                'verbose': 1
            }
            
            target_model = SAC('MlpPolicy', env, **transfer_config)
            
            # 4. ê°€ì¤‘ì¹˜ ì „ì´ ì‹œë„
            transfer_success = self.transfer_mountaincar_weights(pretrained_model, target_model)
            
            if transfer_success:
                logger.info(" MountainCar ê°€ì¤‘ì¹˜ ì „ì´ ì„±ê³µ")
            else:
                logger.info(" ê°€ì¤‘ì¹˜ ì „ì´ ì‹¤íŒ¨, ëœë¤ ì´ˆê¸°í™”ë¡œ ì§„í–‰")
            
            # 5. ë¯¸ì„¸ì¡°ì • í›ˆë ¨
            logger.info("MountainCar ì „ì´í•™ìŠµ ë¯¸ì„¸ì¡°ì • (50k ìŠ¤í…)...")
            target_model.learn(
                total_timesteps=50000,
                progress_bar=True
            )
            
            # 6. ëª¨ë¸ ì €ì¥
            model_path = f"./models/sac_mountaincar_transfer_{self.experiment_id}.zip"
            target_model.save(model_path)
            logger.info(f"ğŸ’¾ MountainCar ëª¨ë¸ ì €ì¥: {model_path}")
            
            return target_model, {
                'transfer_success': transfer_success,
                'model_path': model_path,
                'training_steps': 50000
            }
            
        except Exception as e:
            logger.error(f"MountainCar ì „ì´í•™ìŠµ ì‹¤íŒ¨: {e}")
            return None, {'transfer_success': False, 'error': str(e)}
    
    def transfer_mountaincar_weights(self, source_model, target_model):
        """MountainCar ê°€ì¤‘ì¹˜ ì „ì´ (ë” ì •êµí•œ ë°©ë²•)"""
        
        try:
            source_params = source_model.policy.state_dict()
            target_params = target_model.policy.state_dict()
            
            transferred_layers = 0
            total_layers = len(target_params)
            
            logger.info("ğŸ”„ ê°€ì¤‘ì¹˜ ì „ì´ ì‹œë„ ì¤‘...")
            
            for target_key, target_tensor in target_params.items():
                # í˜¸í™˜ ê°€ëŠ¥í•œ ë ˆì´ì–´ ì°¾ê¸°
                for source_key, source_tensor in source_params.items():
                    if self.is_compatible_layer(source_key, target_key, source_tensor, target_tensor):
                        
                        # ì°¨ì›ì´ ì •í™•íˆ ë§ëŠ” ê²½ìš°
                        if source_tensor.shape == target_tensor.shape:
                            target_params[target_key] = source_tensor.clone()
                            transferred_layers += 1
                            logger.info(f"    ì „ì´: {source_key} â†’ {target_key} {source_tensor.shape}")
                            break
                        
                        # ë¶€ë¶„ í˜¸í™˜ (ì²« ë²ˆì§¸ ì°¨ì›ë§Œ ë§ëŠ” ê²½ìš°)
                        elif (len(source_tensor.shape) == len(target_tensor.shape) and 
                              source_tensor.shape[0] <= target_tensor.shape[0]):
                            
                            # ê°€ëŠ¥í•œ ë¶€ë¶„ë§Œ ì „ì´
                            if len(source_tensor.shape) == 2:
                                target_params[target_key][:source_tensor.shape[0], :source_tensor.shape[1]] = source_tensor
                            elif len(source_tensor.shape) == 1:
                                target_params[target_key][:source_tensor.shape[0]] = source_tensor
                            
                            transferred_layers += 1
                            logger.info(f"   ğŸ”¸ ë¶€ë¶„ ì „ì´: {source_key} â†’ {target_key}")
                            break
            
            # ì—…ë°ì´íŠ¸ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ
            target_model.policy.load_state_dict(target_params)
            
            transfer_rate = transferred_layers / total_layers
            logger.info(f"ì „ì´ ì™„ë£Œ: {transferred_layers}/{total_layers} ({transfer_rate:.1%})")
            
            return transfer_rate > 0.05  # 5% ì´ìƒ ì „ì´ë˜ë©´ ì„±ê³µ
            
        except Exception as e:
            logger.warning(f" ê°€ì¤‘ì¹˜ ì „ì´ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def is_compatible_layer(self, source_key, target_key, source_tensor, target_tensor):
        """ë ˆì´ì–´ í˜¸í™˜ì„± í™•ì¸ (MountainCar íŠ¹í™”)"""
        
        # ìœ ì‚¬í•œ ë ˆì´ì–´ íŒ¨í„´
        compatible_patterns = [
            ('actor.mu', 'actor.mu'),           # Actor ì¶œë ¥
            ('critic.qf', 'critic.qf'),         # Critic Q-function
            ('.0.weight', '.0.weight'),         # ì²« ë²ˆì§¸ ë ˆì´ì–´
            ('.2.weight', '.2.weight'),         # ë‘ ë²ˆì§¸ ë ˆì´ì–´
            ('.0.bias', '.0.bias'),             # ì²« ë²ˆì§¸ ë°”ì´ì–´ìŠ¤
            ('.2.bias', '.2.bias'),             # ë‘ ë²ˆì§¸ ë°”ì´ì–´ìŠ¤
        ]
        
        for source_pattern, target_pattern in compatible_patterns:
            if source_pattern in source_key and target_pattern in target_key:
                return True
        
        return False
    
    def evaluate_mountaincar_model(self, model, env):
        """MountainCar ì „ì´í•™ìŠµ ëª¨ë¸ í‰ê°€"""
        
        logger.info(" MountainCar ëª¨ë¸ í‰ê°€ ì¤‘...")
        
        rewards = []
        efficiency_values = []
        
        for episode in range(30):  # í†µê³„ì  ì‹ ë¢°ì„±ì„ ìœ„í•œ 30íšŒ
            obs, _ = env.reset()
            total_reward = 0
            done = False
            
            while not done:
                action, _ = model.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, _ = env.step(action)
                total_reward += reward
                done = terminated or truncated
            
            rewards.append(total_reward)
            metrics = env.get_episode_metrics()
            efficiency_values.append(metrics['energy_efficiency'])
        
        # í†µê³„ì  ë¶„ì„
        results = {
            'episodes': len(rewards),
            'mean_reward': np.mean(rewards),
            'std_reward': np.std(rewards, ddof=1),
            'mean_efficiency': np.mean(efficiency_values),
            'std_efficiency': np.std(efficiency_values, ddof=1),
            'confidence_interval_95': stats.t.interval(
                0.95, len(efficiency_values)-1,
                loc=np.mean(efficiency_values),
                scale=stats.sem(efficiency_values)
            ),
            'raw_values': {
                'rewards': rewards,
                'efficiencies': efficiency_values
            }
        }
        
        logger.info(f"ğŸ”ï¸ MountainCar ê²°ê³¼:")
        logger.info(f"   í‰ê·  íš¨ìœ¨: {results['mean_efficiency']:.3f} Â± {results['std_efficiency']:.3f} km/kWh")
        logger.info(f"   95% ì‹ ë¢°êµ¬ê°„: [{results['confidence_interval_95'][0]:.3f}, {results['confidence_interval_95'][1]:.3f}]")
        
        return results
    
    def perform_extended_statistical_comparison(self):
        """4ê°œ ëª¨ë¸ í™•ì¥ í†µê³„ ë¹„êµ"""
        
        logger.info("ğŸ“ˆ í™•ì¥ëœ í†µê³„ ë¶„ì„ (4ê°œ ëª¨ë¸)...")
        
        # ì—ë„ˆì§€ íš¨ìœ¨ ê°’ ì¶”ì¶œ
        model_efficiencies = {}
        
        for model_name, result in self.results.items():
            if result and 'mean_efficiency' in result:
                model_efficiencies[model_name] = result['raw_values']['efficiencies']
            elif result and 'energy_efficiency' in result:
                model_efficiencies[model_name] = result['energy_efficiency']['values']
        
        if len(model_efficiencies) < 2:
            logger.error("ë¹„êµí•  ëª¨ë¸ì´ ë¶€ì¡±í•©ë‹ˆë‹¤")
            return None
        
        # í†µê³„ ë¶„ì„
        analysis = {
            'model_means': {},
            'pairwise_comparisons': {},
            'anova_test': None,
            'ranking': [],
            'improvement_rates': {}
        }
        
        # ëª¨ë¸ë³„ í‰ê·  ê³„ì‚°
        for model, values in model_efficiencies.items():
            analysis['model_means'][model] = {
                'mean': np.mean(values),
                'std': np.std(values, ddof=1),
                'n': len(values)
            }
        
        # ìˆœìœ„ ë§¤ê¸°ê¸°
        ranking = sorted(analysis['model_means'].items(), 
                        key=lambda x: x[1]['mean'], reverse=True)
        analysis['ranking'] = [(name, data['mean']) for name, data in ranking]
        
        # ìŒë³„ ë¹„êµ (ëª¨ë“  ì¡°í•©)
        model_names = list(model_efficiencies.keys())
        for i, model1 in enumerate(model_names):
            for j, model2 in enumerate(model_names):
                if i < j:  # ì¤‘ë³µ ë°©ì§€
                    values1 = model_efficiencies[model1]
                    values2 = model_efficiencies[model2]
                    
                    # t-ê²€ì •
                    t_stat, t_p = stats.ttest_ind(values1, values2)
                    
                    # Mann-Whitney U ê²€ì • (ë¹„ëª¨ìˆ˜)
                    u_stat, u_p = stats.mannwhitneyu(values1, values2, alternative='two-sided')
                    
                    # íš¨ê³¼í¬ê¸° (Cohen's d)
                    cohens_d = self.calculate_cohens_d(values1, values2)
                    
                    comparison_key = f"{model1}_vs_{model2}"
                    analysis['pairwise_comparisons'][comparison_key] = {
                        't_test': {'statistic': t_stat, 'p_value': t_p, 'significant': t_p < 0.05},
                        'mann_whitney': {'statistic': u_stat, 'p_value': u_p, 'significant': u_p < 0.05},
                        'effect_size': cohens_d,
                        'mean_difference': np.mean(values1) - np.mean(values2)
                    }
        
        # ANOVA (3ê°œ ì´ìƒ ëª¨ë¸ì´ ìˆëŠ” ê²½ìš°)
        if len(model_efficiencies) >= 3:
            anova_values = list(model_efficiencies.values())
            f_stat, anova_p = stats.f_oneway(*anova_values)
            analysis['anova_test'] = {
                'f_statistic': f_stat,
                'p_value': anova_p,
                'significant': anova_p < 0.05
            }
        
        # í¬ë£¨ì¦ˆ ëª¨ë“œ ëŒ€ë¹„ ê°œì„ ìœ¨ ê³„ì‚°
        if 'cruise_mode' in analysis['model_means']:
            cruise_mean = analysis['model_means']['cruise_mode']['mean']
            
            for model, data in analysis['model_means'].items():
                if model != 'cruise_mode':
                    improvement = ((data['mean'] - cruise_mean) / cruise_mean) * 100
                    analysis['improvement_rates'][model] = improvement
        
        return analysis
    
    def calculate_cohens_d(self, group1, group2):
        """Cohen's d íš¨ê³¼í¬ê¸° ê³„ì‚°"""
        n1, n2 = len(group1), len(group2)
        pooled_std = np.sqrt(((n1 - 1) * np.var(group1, ddof=1) + 
                             (n2 - 1) * np.var(group2, ddof=1)) / (n1 + n2 - 2))
        return (np.mean(group1) - np.mean(group2)) / pooled_std
    
    def create_comprehensive_visualizations(self, statistical_analysis):
        """sagemaker_test.py ìˆ˜ì¤€ì˜ ì¢…í•© ì‹œê°í™” ìƒì„±"""
        
        logger.info(" ì¢…í•© ì‹œê°í™” ìƒì„± ì¤‘ (sagemaker_test.py ìˆ˜ì¤€)...")
        
        import matplotlib.pyplot as plt
        import seaborn as sns
        from matplotlib.gridspec import GridSpec
        
        # ìŠ¤íƒ€ì¼ ì„¤ì •
        plt.style.use('default')
        sns.set_palette("husl")
        
        # ëª¨ë¸ í‘œì‹œëª… ë§¤í•‘
        model_display_names = {
            'cruise_mode': 'í¬ë£¨ì¦ˆ ëª¨ë“œ',
            'sac_scratch': 'SAC ìˆœìˆ˜í•™ìŠµ',
            'sac_lunarlander': 'SAC LunarLander ì „ì´',
            'sac_mountaincar': 'SAC MountainCar ì „ì´'
        }
        
        # 1. ë©”ì¸ ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸ (2x2 ë ˆì´ì•„ì›ƒ)
        fig = plt.figure(figsize=(20, 16))
        gs = GridSpec(3, 2, figure=fig, height_ratios=[1, 1, 1], hspace=0.3, wspace=0.3)
        
        fig.suptitle('4ê°œ ëª¨ë¸ ì¢…í•© ì„±ëŠ¥ ë¹„êµ\nMountainCar vs LunarLander vs ìˆœìˆ˜í•™ìŠµ vs í¬ë£¨ì¦ˆ', 
                    fontsize=20, fontweight='bold', y=0.95)
        
        # 1-1. ì—ë„ˆì§€ íš¨ìœ¨ ë°•ìŠ¤í”Œë¡¯
        ax1 = fig.add_subplot(gs[0, 0])
        efficiency_data = []
        labels = []
        
        for model_name in statistical_analysis['ranking']:
            model_key = model_name[0]
            if model_key in self.results and self.results[model_key]:
                if 'raw_values' in self.results[model_key]:
                    efficiency_data.append(self.results[model_key]['raw_values']['efficiencies'])
                elif 'energy_efficiency' in self.results[model_key]:
                    efficiency_data.append(self.results[model_key]['energy_efficiency']['values'])
                labels.append(model_display_names.get(model_key, model_key))
        
        if efficiency_data:
            box_plot = ax1.boxplot(efficiency_data, labels=labels, patch_artist=True)
            colors = ['lightcoral', 'lightblue', 'lightgreen', 'orange']
            for patch, color in zip(box_plot['boxes'], colors[:len(box_plot['boxes'])]):
                patch.set_facecolor(color)
                patch.set_alpha(0.7)
        
        ax1.set_title('ì—ë„ˆì§€ íš¨ìœ¨ ë¶„í¬ ë¹„êµ', fontweight='bold', fontsize=14)
        ax1.set_ylabel('ì—ë„ˆì§€ íš¨ìœ¨ (km/kWh)', fontsize=12)
        ax1.grid(True, alpha=0.3)
        ax1.tick_params(axis='x', rotation=45)
        
        # í‰ê· ê°’ í‘œì‹œ
        for i, data in enumerate(efficiency_data):
            mean_val = np.mean(data)
            ax1.text(i+1, mean_val + 0.05, f'{mean_val:.2f}', 
                    ha='center', va='bottom', fontweight='bold', fontsize=10)
        
        # 1-2. ì„±ëŠ¥ ìˆœìœ„ ë§‰ëŒ€ì°¨íŠ¸
        ax2 = fig.add_subplot(gs[0, 1])
        models = [model_display_names.get(name[0], name[0]) for name in statistical_analysis['ranking']]
        efficiencies = [eff for _, eff in statistical_analysis['ranking']]
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']
        
        bars = ax2.bar(models, efficiencies, color=colors[:len(models)], alpha=0.8)
        ax2.set_title('ì„±ëŠ¥ ìˆœìœ„ (ì—ë„ˆì§€ íš¨ìœ¨)', fontweight='bold', fontsize=14)
        ax2.set_ylabel('ì—ë„ˆì§€ íš¨ìœ¨ (km/kWh)', fontsize=12)
        ax2.tick_params(axis='x', rotation=45)
        ax2.grid(True, alpha=0.3)
        
        # ìˆœìœ„ í‘œì‹œ
        for i, (bar, eff) in enumerate(zip(bars, efficiencies)):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.02,
                    f'{i+1}ìœ„\n{eff:.3f}', ha='center', va='bottom', 
                    fontweight='bold', fontsize=10)
        
        # 1-3. ê°œì„ ìœ¨ ë¹„êµ
        ax3 = fig.add_subplot(gs[1, 0])
        if statistical_analysis['improvement_rates']:
            improvement_models = []
            improvement_values = []
            
            for model, improvement in statistical_analysis['improvement_rates'].items():
                improvement_models.append(model_display_names.get(model, model))
                improvement_values.append(improvement)
            
            colors = ['green' if x >= 20 else 'orange' if x >= 10 else 'red' 
                     for x in improvement_values]
            
            bars = ax3.bar(improvement_models, improvement_values, color=colors, alpha=0.8)
            ax3.axhline(y=20, color='red', linestyle='--', linewidth=2, label='ëª©í‘œ 20% ê°œì„ ')
            ax3.set_title('í¬ë£¨ì¦ˆ ëª¨ë“œ ëŒ€ë¹„ ê°œì„ ìœ¨', fontweight='bold', fontsize=14)
            ax3.set_ylabel('ê°œì„ ìœ¨ (%)', fontsize=12)
            ax3.tick_params(axis='x', rotation=45)
            ax3.legend()
            ax3.grid(True, alpha=0.3)
            
            # ê°œì„ ìœ¨ ê°’ í‘œì‹œ
            for bar, improvement in zip(bars, improvement_values):
                height = bar.get_height()
                status = '' if improvement >= 20 else '' if improvement >= 10 else 'âŒ'
                ax3.text(bar.get_x() + bar.get_width()/2., 
                        height + (1 if height > 0 else -3),
                        f'{status}\n{improvement:.1f}%', 
                        ha='center', va='bottom' if height > 0 else 'top',
                        fontweight='bold', fontsize=10)
        
        # 1-4. í†µê³„ì  ìœ ì˜ì„± íˆíŠ¸ë§µ
        ax4 = fig.add_subplot(gs[1, 1])
        
        # p-value ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
        model_names = list(statistical_analysis['model_means'].keys())
        n_models = len(model_names)
        p_matrix = np.ones((n_models, n_models))
        
        for i, model1 in enumerate(model_names):
            for j, model2 in enumerate(model_names):
                if i != j:
                    comparison_key = f"{model1}_vs_{model2}"
                    reverse_key = f"{model2}_vs_{model1}"
                    
                    if comparison_key in statistical_analysis['pairwise_comparisons']:
                        p_val = statistical_analysis['pairwise_comparisons'][comparison_key]['t_test']['p_value']
                        p_matrix[i, j] = p_val
                    elif reverse_key in statistical_analysis['pairwise_comparisons']:
                        p_val = statistical_analysis['pairwise_comparisons'][reverse_key]['t_test']['p_value']
                        p_matrix[i, j] = p_val
        
        # íˆíŠ¸ë§µ ìƒì„±
        mask = np.eye(n_models, dtype=bool)
        display_names = [model_display_names.get(name, name) for name in model_names]
        
        sns.heatmap(p_matrix, mask=mask, annot=True, fmt='.3f', 
                   xticklabels=display_names, yticklabels=display_names,
                   cmap='RdYlBu_r', center=0.05, ax=ax4,
                   cbar_kws={'label': 'p-value'})
        ax4.set_title('í†µê³„ì  ìœ ì˜ì„± (p-values)', fontweight='bold', fontsize=14)
        
        # 1-5. íš¨ê³¼í¬ê¸° ë¹„êµ
        ax5 = fig.add_subplot(gs[2, 0])
        
        effect_sizes = []
        comparison_labels = []
        
        for key, comparison in statistical_analysis['pairwise_comparisons'].items():
            if 'vs' in key:
                model1, model2 = key.split('_vs_')
                label = f"{model_display_names.get(model1, model1)}\nvs\n{model_display_names.get(model2, model2)}"
                comparison_labels.append(label)
                effect_sizes.append(abs(comparison['effect_size']))
        
        if effect_sizes:
            colors = ['green' if x >= 0.8 else 'orange' if x >= 0.5 else 'yellow' if x >= 0.2 else 'red' 
                     for x in effect_sizes]
            
            bars = ax5.bar(range(len(effect_sizes)), effect_sizes, color=colors, alpha=0.8)
            ax5.set_xticks(range(len(comparison_labels)))
            ax5.set_xticklabels(comparison_labels, fontsize=8)
            ax5.set_title('íš¨ê³¼í¬ê¸° (Cohen\'s d)', fontweight='bold', fontsize=14)
            ax5.set_ylabel('íš¨ê³¼í¬ê¸°', fontsize=12)
            ax5.axhline(y=0.2, color='gray', linestyle='--', alpha=0.5, label='ì†Œ íš¨ê³¼ (0.2)')
            ax5.axhline(y=0.5, color='gray', linestyle='--', alpha=0.7, label='ì¤‘ íš¨ê³¼ (0.5)')
            ax5.axhline(y=0.8, color='gray', linestyle='--', alpha=0.9, label='ëŒ€ íš¨ê³¼ (0.8)')
            ax5.legend(fontsize=8)
            ax5.grid(True, alpha=0.3)
            
            # íš¨ê³¼í¬ê¸° ê°’ í‘œì‹œ
            for bar, effect in zip(bars, effect_sizes):
                height = bar.get_height()
                if effect >= 0.8:
                    effect_desc = 'ëŒ€'
                elif effect >= 0.5:
                    effect_desc = 'ì¤‘'
                elif effect >= 0.2:
                    effect_desc = 'ì†Œ'
                else:
                    effect_desc = 'ë¯¸ë¯¸'
                
                ax5.text(bar.get_x() + bar.get_width()/2., height + 0.02,
                        f'{effect:.2f}\n({effect_desc})', ha='center', va='bottom', 
                        fontweight='bold', fontsize=9)
        
        # 1-6. ê°€ì„¤ ê²€ì¦ ê²°ê³¼
        ax6 = fig.add_subplot(gs[2, 1])
        
        # ê°€ì„¤ ê²€ì¦ í•­ëª©ë“¤
        hypotheses = []
        results = []
        
        # H1: ì „ì´í•™ìŠµ ìš°ìˆ˜ì„±
        if ('sac_mountaincar' in statistical_analysis['model_means'] and 
            'sac_lunarlander' in statistical_analysis['model_means']):
            mountain_eff = statistical_analysis['model_means']['sac_mountaincar']['mean']
            lunar_eff = statistical_analysis['model_means']['sac_lunarlander']['mean']
            
            hypotheses.append('H1: MountainCar\n> LunarLander')
            results.append(1 if mountain_eff > lunar_eff else 0)
        
        # H2: 20% ê°œì„  ë‹¬ì„±
        for model in ['sac_scratch', 'sac_lunarlander', 'sac_mountaincar']:
            if model in statistical_analysis['improvement_rates']:
                improvement = statistical_analysis['improvement_rates'][model]
                model_name = model_display_names.get(model, model)
                hypotheses.append(f'H2: {model_name}\nâ‰¥20% ê°œì„ ')
                results.append(1 if improvement >= 20 else 0)
        
        if hypotheses:
            colors = ['green' if x == 1 else 'red' for x in results]
            bars = ax6.bar(hypotheses, results, color=colors, alpha=0.8)
            ax6.set_title('ê°€ì„¤ ê²€ì¦ ê²°ê³¼', fontweight='bold', fontsize=14)
            ax6.set_ylabel('ë‹¬ì„± ì—¬ë¶€ (1=ì„±ê³µ, 0=ì‹¤íŒ¨)', fontsize=12)
            ax6.set_ylim(0, 1.2)
            ax6.tick_params(axis='x', rotation=45)
            ax6.grid(True, alpha=0.3)
            
            # ê²°ê³¼ í…ìŠ¤íŠ¸ í‘œì‹œ
            for bar, result in zip(bars, results):
                text = ' ë‹¬ì„±' if result else 'ë¯¸ë‹¬ì„±'
                ax6.text(bar.get_x() + bar.get_width()/2., 0.5, text, 
                        ha='center', va='center', fontweight='bold', fontsize=10,
                        color='white')
        
        plt.tight_layout()
        plt.savefig(f'{self.results_dir}/comprehensive_4model_comparison.png', 
                   dpi=300, bbox_inches='tight')
        plt.show()
        
        # 2. ìƒì„¸ í†µê³„ ë¶„ì„ ì°¨íŠ¸
        self.create_detailed_statistical_charts(statistical_analysis)
        
        # 3. ì „ì´í•™ìŠµ íš¨ê³¼ ë¶„ì„ ì°¨íŠ¸
        self.create_transfer_learning_analysis_charts(statistical_analysis)
        
        logger.info(" ì¢…í•© ì‹œê°í™” ì™„ë£Œ")
    
    def create_detailed_statistical_charts(self, statistical_analysis):
        """ìƒì„¸ í†µê³„ ë¶„ì„ ì°¨íŠ¸"""
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('ìƒì„¸ í†µê³„ ë¶„ì„ ê²°ê³¼', fontsize=16, fontweight='bold')
        
        model_display_names = {
            'cruise_mode': 'í¬ë£¨ì¦ˆ ëª¨ë“œ',
            'sac_scratch': 'SAC ìˆœìˆ˜í•™ìŠµ',
            'sac_lunarlander': 'SAC LunarLander ì „ì´',
            'sac_mountaincar': 'SAC MountainCar ì „ì´'
        }
        
        # 2-1. ì‹ ë¢°êµ¬ê°„ ë¹„êµ
        models = []
        means = []
        ci_lowers = []
        ci_uppers = []
        
        for model_name, result in self.results.items():
            if result and 'confidence_interval_95' in result:
                models.append(model_display_names.get(model_name, model_name))
                means.append(result['mean_efficiency'])
                ci_lower, ci_upper = result['confidence_interval_95']
                ci_lowers.append(ci_lower)
                ci_uppers.append(ci_upper)
        
        if models:
            x_pos = np.arange(len(models))
            axes[0, 0].errorbar(x_pos, means, 
                              yerr=[np.array(means) - np.array(ci_lowers),
                                    np.array(ci_uppers) - np.array(means)],
                              fmt='o', capsize=5, capthick=2, markersize=8)
            axes[0, 0].set_xticks(x_pos)
            axes[0, 0].set_xticklabels(models, rotation=45)
            axes[0, 0].set_title('95% ì‹ ë¢°êµ¬ê°„ ë¹„êµ', fontweight='bold')
            axes[0, 0].set_ylabel('ì—ë„ˆì§€ íš¨ìœ¨ (km/kWh)')
            axes[0, 0].grid(True, alpha=0.3)
        
        # 2-2. ë¶„ì‚° ë¹„êµ
        models = []
        variances = []
        
        for model_name, data in statistical_analysis['model_means'].items():
            models.append(model_display_names.get(model_name, model_name))
            variances.append(data['std']**2)
        
        if models:
            bars = axes[0, 1].bar(models, variances, alpha=0.7, color='skyblue')
            axes[0, 1].set_title('ë¶„ì‚° ë¹„êµ (ì•ˆì •ì„±)', fontweight='bold')
            axes[0, 1].set_ylabel('ë¶„ì‚°')
            axes[0, 1].tick_params(axis='x', rotation=45)
            axes[0, 1].grid(True, alpha=0.3)
            
            for bar, var in zip(bars, variances):
                height = bar.get_height()
                axes[0, 1].text(bar.get_x() + bar.get_width()/2., height + 0.001,
                               f'{var:.3f}', ha='center', va='bottom', fontweight='bold')
        
        # 2-3. ì •ê·œì„± ê²€ì • ê²°ê³¼ (ë§Œì•½ ìˆë‹¤ë©´)
        axes[0, 2].text(0.5, 0.5, 'Normality Tests\n(if available)', 
                       ha='center', va='center', fontsize=12, 
                       transform=axes[0, 2].transAxes)
        axes[0, 2].set_title('ì •ê·œì„± ê²€ì •', fontweight='bold')
        
        # 2-4. ANOVA ê²°ê³¼
        if statistical_analysis['anova_test']:
            anova = statistical_analysis['anova_test']
            
            labels = ['F-í†µê³„ëŸ‰', 'p-ê°’']
            values = [anova['f_statistic'], anova['p_value']]
            
            bars = axes[0, 3].bar(labels, values, color=['orange', 'lightgreen'], alpha=0.7)
            axes[0, 3].set_title('ì¼ì›ë°°ì¹˜ ë¶„ì‚°ë¶„ì„ (ANOVA)', fontweight='bold')
            axes[0, 3].grid(True, alpha=0.3)
            
            for bar, val in zip(bars, values):
                height = bar.get_height()
                axes[0, 3].text(bar.get_x() + bar.get_width()/2., height + 0.01,
                               f'{val:.4f}', ha='center', va='bottom', fontweight='bold')
            
            # ìœ ì˜ì„± í‘œì‹œ
            significance = 'ìœ ì˜í•¨' if anova['significant'] else 'ë¹„ìœ ì˜í•¨'
            axes[0, 3].text(0.5, 0.8, f'ê²°ê³¼: {significance}', 
                           ha='center', va='center', fontsize=12, fontweight='bold',
                           transform=axes[0, 3].transAxes)
        
        plt.tight_layout()
        plt.savefig(f'{self.results_dir}/detailed_statistical_analysis.png', 
                   dpi=300, bbox_inches='tight')
        plt.show()
    
    def create_transfer_learning_analysis_charts(self, statistical_analysis):
        """ì „ì´í•™ìŠµ íš¨ê³¼ ë¶„ì„ ì°¨íŠ¸"""
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('ì „ì´í•™ìŠµ íš¨ê³¼ ë¶„ì„', fontsize=16, fontweight='bold')
        
        # 3-1. ì „ì´í•™ìŠµ vs ìˆœìˆ˜í•™ìŠµ ë¹„êµ
        transfer_models = []
        transfer_efficiencies = []
        
        if 'sac_scratch' in statistical_analysis['model_means']:
            scratch_eff = statistical_analysis['model_means']['sac_scratch']['mean']
            
            for model in ['sac_lunarlander', 'sac_mountaincar']:
                if model in statistical_analysis['model_means']:
                    transfer_models.append('LunarLander' if 'lunar' in model else 'MountainCar')
                    transfer_efficiencies.append(statistical_analysis['model_means'][model]['mean'])
            
            if transfer_models:
                colors = ['lightblue' if eff > scratch_eff else 'lightcoral' for eff in transfer_efficiencies]
                bars = axes[0, 0].bar(transfer_models, transfer_efficiencies, 
                                    color=colors, alpha=0.8)
                axes[0, 0].axhline(y=scratch_eff, color='red', linestyle='--', 
                                 label=f'ìˆœìˆ˜í•™ìŠµ: {scratch_eff:.3f}')
                axes[0, 0].set_title('ì „ì´í•™ìŠµ ëª¨ë¸ vs ìˆœìˆ˜í•™ìŠµ', fontweight='bold')
                axes[0, 0].set_ylabel('ì—ë„ˆì§€ íš¨ìœ¨ (km/kWh)')
                axes[0, 0].legend()
                axes[0, 0].grid(True, alpha=0.3)
                
                for bar, eff in zip(bars, transfer_efficiencies):
                    height = bar.get_height()
                    improvement = ((eff - scratch_eff) / scratch_eff) * 100
                    status = 'â†‘' if improvement > 0 else 'â†“'
                    axes[0, 0].text(bar.get_x() + bar.get_width()/2., height + 0.02,
                                   f'{eff:.3f}\n{status}{abs(improvement):.1f}%', 
                                   ha='center', va='bottom', fontweight='bold')
        
        # 3-2. ë„ë©”ì¸ ìœ ì‚¬ì„± ë¶„ì„
        domain_similarity = {
            'LunarLander': {'ë¬¼ë¦¬ë²•ì¹™': 8, 'ì œì–´ë°©ì‹': 9, 'í™˜ê²½ë³µì¡ë„': 6, 'ëª©í‘œìœ ì‚¬ì„±': 5},
            'MountainCar': {'ë¬¼ë¦¬ë²•ì¹™': 9, 'ì œì–´ë°©ì‹': 8, 'í™˜ê²½ë³µì¡ë„': 4, 'ëª©í‘œìœ ì‚¬ì„±': 8}
        }
        
        categories = list(domain_similarity['LunarLander'].keys())
        lunar_scores = list(domain_similarity['LunarLander'].values())
        mountain_scores = list(domain_similarity['MountainCar'].values())
        
        x = np.arange(len(categories))
        width = 0.35
        
        bars1 = axes[0, 1].bar(x - width/2, lunar_scores, width, label='LunarLander', alpha=0.8)
        bars2 = axes[0, 1].bar(x + width/2, mountain_scores, width, label='MountainCar', alpha=0.8)
        
        axes[0, 1].set_title('ë„ë©”ì¸ ìœ ì‚¬ì„± ë¶„ì„ (ì£¼ê´€ì  í‰ê°€)', fontweight='bold')
        axes[0, 1].set_ylabel('ìœ ì‚¬ì„± ì ìˆ˜ (1-10)')
        axes[0, 1].set_xticks(x)
        axes[0, 1].set_xticklabels(categories, rotation=45)
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3-3. í•™ìŠµ íš¨ìœ¨ì„± (ìŠ¤í…ë‹¹ ì„±ëŠ¥)
        training_data = {
            'SAC ìˆœìˆ˜í•™ìŠµ': {'ìŠ¤í…': 100000, 'íš¨ìœ¨': 0},
            'SAC LunarLander ì „ì´': {'ìŠ¤í…': 50000, 'íš¨ìœ¨': 0},
            'SAC MountainCar ì „ì´': {'ìŠ¤í…': 50000, 'íš¨ìœ¨': 0}
        }
        
        # íš¨ìœ¨ ë°ì´í„° ì±„ìš°ê¸°
        for model_key, model_name in [('sac_scratch', 'SAC ìˆœìˆ˜í•™ìŠµ'), 
                                     ('sac_lunarlander', 'SAC LunarLander ì „ì´'),
                                     ('sac_mountaincar', 'SAC MountainCar ì „ì´')]:
            if model_key in statistical_analysis['model_means']:
                training_data[model_name]['íš¨ìœ¨'] = statistical_analysis['model_means'][model_key]['mean']
        
        models = list(training_data.keys())
        steps = [training_data[model]['ìŠ¤í…'] for model in models]
        efficiencies = [training_data[model]['íš¨ìœ¨'] for model in models]
        efficiency_per_step = [eff/step*1000 if step > 0 else 0 for eff, step in zip(efficiencies, steps)]
        
        bars = axes[1, 0].bar(models, efficiency_per_step, 
                             color=['orange', 'lightblue', 'lightgreen'], alpha=0.8)
        axes[1, 0].set_title('í•™ìŠµ íš¨ìœ¨ì„± (1000ìŠ¤í…ë‹¹ ì„±ëŠ¥)', fontweight='bold')
        axes[1, 0].set_ylabel('íš¨ìœ¨/1000ìŠ¤í…')
        axes[1, 0].tick_params(axis='x', rotation=45)
        axes[1, 0].grid(True, alpha=0.3)
        
        for bar, eps in zip(bars, efficiency_per_step):
            height = bar.get_height()
            axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + 0.0001,
                           f'{eps:.4f}', ha='center', va='bottom', fontweight='bold')
        
        # 3-4. ì „ì´í•™ìŠµ ì„±ê³µë¥  ë¶„ì„
        transfer_success_data = {
            'LunarLander': {'ê°€ì¤‘ì¹˜_ì „ì´ìœ¨': 6.2, 'ìµœì¢…_ì„±ëŠ¥': 0, 'ê°œì„ ì—¬ë¶€': False},
            'MountainCar': {'ê°€ì¤‘ì¹˜_ì „ì´ìœ¨': 0, 'ìµœì¢…_ì„±ëŠ¥': 0, 'ê°œì„ ì—¬ë¶€': False}  # ì‹¤ì œ ê²°ê³¼ë¡œ ì—…ë°ì´íŠ¸
        }
        
        # ì‹¤ì œ ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸
        if 'sac_lunarlander' in statistical_analysis['model_means']:
            transfer_success_data['LunarLander']['ìµœì¢…_ì„±ëŠ¥'] = statistical_analysis['model_means']['sac_lunarlander']['mean']
            if 'sac_scratch' in statistical_analysis['model_means']:
                scratch_perf = statistical_analysis['model_means']['sac_scratch']['mean']
                transfer_success_data['LunarLander']['ê°œì„ ì—¬ë¶€'] = transfer_success_data['LunarLander']['ìµœì¢…_ì„±ëŠ¥'] > scratch_perf
        
        if 'sac_mountaincar' in statistical_analysis['model_means']:
            transfer_success_data['MountainCar']['ìµœì¢…_ì„±ëŠ¥'] = statistical_analysis['model_means']['sac_mountaincar']['mean']
            if 'sac_scratch' in statistical_analysis['model_means']:
                scratch_perf = statistical_analysis['model_means']['sac_scratch']['mean']
                transfer_success_data['MountainCar']['ê°œì„ ì—¬ë¶€'] = transfer_success_data['MountainCar']['ìµœì¢…_ì„±ëŠ¥'] > scratch_perf
        
        # ì„±ê³µë¥  ë§‰ëŒ€ì°¨íŠ¸
        models = list(transfer_success_data.keys())
        success_scores = []
        
        for model, data in transfer_success_data.items():
            # ì¢…í•© ì„±ê³µ ì ìˆ˜ (ê°€ì¤‘ì¹˜ ì „ì´ + ì„±ëŠ¥ ê°œì„ )
            weight_score = min(data['ê°€ì¤‘ì¹˜_ì „ì´ìœ¨'] / 10, 1.0)  # 10% ê¸°ì¤€ìœ¼ë¡œ ì •ê·œí™”
            performance_score = 1.0 if data['ê°œì„ ì—¬ë¶€'] else 0.0
            total_score = (weight_score * 0.3 + performance_score * 0.7) * 100
            success_scores.append(total_score)
        
        colors = ['green' if score >= 70 else 'orange' if score >= 40 else 'red' for score in success_scores]
        bars = axes[1, 1].bar(models, success_scores, color=colors, alpha=0.8)
        axes[1, 1].set_title('ì „ì´í•™ìŠµ ì¢…í•© ì„±ê³µë¥ ', fontweight='bold')
        axes[1, 1].set_ylabel('ì„±ê³µë¥  (%)')
        axes[1, 1].set_ylim(0, 100)
        axes[1, 1].grid(True, alpha=0.3)
        
        for bar, score in zip(bars, success_scores):
            height = bar.get_height()
            status = 'ì„±ê³µ' if score >= 70 else 'ë¶€ë¶„ì„±ê³µ' if score >= 40 else 'ì‹¤íŒ¨'
            axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 2,
                           f'{score:.1f}%\n({status})', ha='center', va='bottom', 
                           fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(f'{self.results_dir}/transfer_learning_analysis.png', 
                   dpi=300, bbox_inches='tight')
        plt.show()
    
    def create_publication_ready_tables(self, statistical_analysis):
        """ë…¼ë¬¸ìš© LaTeX í…Œì´ë¸” ìƒì„±"""
        
        logger.info("ğŸ“‹ ë…¼ë¬¸ìš© í…Œì´ë¸” ìƒì„±...")
        
        model_display_names = {
            'cruise_mode': 'Cruise Mode',
            'sac_scratch': 'SAC From Scratch',
            'sac_lunarlander': 'SAC + LunarLander Transfer',
            'sac_mountaincar': 'SAC + MountainCar Transfer'
        }
        
        # 1. ë©”ì¸ ì„±ëŠ¥ ë¹„êµ í…Œì´ë¸”
        performance_table = """\\begin{table}[htbp]
\\centering
\\caption{Performance Comparison of Four Models}
\\label{tab:performance_comparison}
\\begin{tabular}{lcccc}
\\toprule
Model & Energy Efficiency & 95\\% CI & Improvement & Rank \\\\
& (km/kWh) & (km/kWh) & (\\%) & \\\\
\\midrule
"""
        
        for i, (model_name, efficiency) in enumerate(statistical_analysis['ranking'], 1):
            model_display = model_display_names.get(model_name, model_name)
            
            # ì‹ ë¢°êµ¬ê°„ ì •ë³´
            if model_name in self.results and 'confidence_interval_95' in self.results[model_name]:
                ci = self.results[model_name]['confidence_interval_95']
                ci_str = f"[{ci[0]:.3f}, {ci[1]:.3f}]"
            else:
                ci_str = "N/A"
            
            # ê°œì„ ìœ¨
            if model_name in statistical_analysis['improvement_rates']:
                improvement = statistical_analysis['improvement_rates'][model_name]
                improvement_str = f"{improvement:+.1f}"
            else:
                improvement_str = "Baseline"
            
            performance_table += f"{model_display} & {efficiency:.3f} & {ci_str} & {improvement_str} & {i} \\\\\n"
        
        performance_table += """\\bottomrule
\\end{tabular}
\\end{table}

"""
        
        # 2. í†µê³„ì  ê²€ì • ê²°ê³¼ í…Œì´ë¸”
        statistical_table = """\\begin{table}[htbp]
\\centering
\\caption{Statistical Significance Tests (Pairwise Comparisons)}
\\label{tab:statistical_tests}
\\begin{tabular}{lcccr}
\\toprule
Comparison & t-statistic & p-value & Cohen's d & Significance \\\\
\\midrule
"""
        
        for comparison_name, comparison_data in statistical_analysis['pairwise_comparisons'].items():
            model1, model2 = comparison_name.split('_vs_')
            model1_display = model_display_names.get(model1, model1)
            model2_display = model_display_names.get(model2, model2)
            
            t_stat = comparison_data['t_test']['statistic']
            p_val = comparison_data['t_test']['p_value']
            cohens_d = comparison_data['effect_size']
            significance = "***" if p_val < 0.001 else "**" if p_val < 0.01 else "*" if p_val < 0.05 else "ns"
            
            comparison_label = f"{model1_display} vs {model2_display}"
            statistical_table += f"{comparison_label} & {t_stat:.3f} & {p_val:.4f} & {cohens_d:.3f} & {significance} \\\\\n"
        
        statistical_table += """\\bottomrule
\\end{tabular}
\\note{*** p < 0.001, ** p < 0.01, * p < 0.05, ns = not significant}
\\end{table}

"""
        
        # 3. ì „ì´í•™ìŠµ íš¨ê³¼ ë¶„ì„ í…Œì´ë¸”
        transfer_table = """\\begin{table}[htbp]
\\centering
\\caption{Transfer Learning Effectiveness Analysis}
\\label{tab:transfer_learning}
\\begin{tabular}{lcccc}
\\toprule
Transfer Model & Training Steps & Weight Transfer & Final Performance & vs Scratch \\\\
& & Rate (\\%) & (km/kWh) & Improvement \\\\
\\midrule
"""
        
        transfer_models = [
            ('sac_lunarlander', 'LunarLander', 50000, 6.2),
            ('sac_mountaincar', 'MountainCar', 50000, 0)  # ì‹¤ì œ ì „ì´ìœ¨ë¡œ ì—…ë°ì´íŠ¸ í•„ìš”
        ]
        
        scratch_performance = statistical_analysis['model_means'].get('sac_scratch', {}).get('mean', 0)
        
        for model_key, model_name, steps, transfer_rate in transfer_models:
            if model_key in statistical_analysis['model_means']:
                performance = statistical_analysis['model_means'][model_key]['mean']
                vs_scratch = ((performance - scratch_performance) / scratch_performance) * 100 if scratch_performance > 0 else 0
                vs_scratch_str = f"{vs_scratch:+.1f}\\%"
                
                transfer_table += f"{model_name} & {steps:,} & {transfer_rate:.1f} & {performance:.3f} & {vs_scratch_str} \\\\\n"
        
        transfer_table += """\\bottomrule
\\end{tabular}
\\end{table}

"""
        
        # 4. ANOVA ê²°ê³¼ í…Œì´ë¸”
        anova_table = ""
        if statistical_analysis['anova_test']:
            anova = statistical_analysis['anova_test']
            anova_table = f"""\\begin{table}[htbp]
\\centering
\\caption{{One-way ANOVA Results}}
\\label{{tab:anova}}
\\begin{{tabular}}{{lcc}}
\\toprule
Source & F-statistic & p-value \\\\
\\midrule
Between Groups & {anova['f_statistic']:.3f} & {anova['p_value']:.4f} \\\\
\\bottomrule
\\end{{tabular}}
\\note{{Null hypothesis: All group means are equal}}
\\end{{table}}

"""
        
        # ì „ì²´ LaTeX í…Œì´ë¸” ì €ì¥
        full_latex = performance_table + statistical_table + transfer_table + anova_table
        
        with open(f'{self.results_dir}/publication_tables.tex', 'w', encoding='utf-8') as f:
            f.write(full_latex)
        
        # CSV í˜•íƒœë¡œë„ ì €ì¥ (ìŠ¤í”„ë ˆë“œì‹œíŠ¸ í˜¸í™˜)
        self.create_csv_tables(statistical_analysis)
        
        logger.info(" ë…¼ë¬¸ìš© í…Œì´ë¸” ìƒì„± ì™„ë£Œ")
        logger.info(f"  - LaTeX: {self.results_dir}/publication_tables.tex")
        logger.info(f"  - CSV: {self.results_dir}/results_tables.csv")
    
    def create_csv_tables(self, statistical_analysis):
        """CSV í˜•íƒœ í…Œì´ë¸” ìƒì„±"""
        
        import csv
        
        # ì„±ëŠ¥ ë¹„êµ CSV
        with open(f'{self.results_dir}/performance_comparison.csv', 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['Model', 'Energy_Efficiency_kmkWh', 'Std_Dev', 'CI_Lower', 'CI_Upper', 'Improvement_Percent', 'Rank'])
            
            for i, (model_name, efficiency) in enumerate(statistical_analysis['ranking'], 1):
                model_data = statistical_analysis['model_means'][model_name]
                
                # ì‹ ë¢°êµ¬ê°„
                if model_name in self.results and 'confidence_interval_95' in self.results[model_name]:
                    ci_lower, ci_upper = self.results[model_name]['confidence_interval_95']
                else:
                    ci_lower, ci_upper = '', ''
                
                # ê°œì„ ìœ¨
                improvement = statistical_analysis['improvement_rates'].get(model_name, 0)
                
                writer.writerow([
                    model_name, 
                    f"{efficiency:.4f}", 
                    f"{model_data['std']:.4f}",
                    f"{ci_lower:.4f}" if ci_lower != '' else '',
                    f"{ci_upper:.4f}" if ci_upper != '' else '',
                    f"{improvement:.2f}" if improvement != 0 else '',
                    i
                ])
        
        # í†µê³„ ê²€ì • CSV
        with open(f'{self.results_dir}/statistical_tests.csv', 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['Comparison', 'T_Statistic', 'P_Value', 'Cohens_D', 'Significant', 'Effect_Size_Category'])
            
            for comparison_name, comparison_data in statistical_analysis['pairwise_comparisons'].items():
                t_stat = comparison_data['t_test']['statistic']
                p_val = comparison_data['t_test']['p_value']
                cohens_d = comparison_data['effect_size']
                significant = 'Yes' if comparison_data['t_test']['significant'] else 'No'
                
                # íš¨ê³¼í¬ê¸° ë¶„ë¥˜
                abs_d = abs(cohens_d)
                if abs_d >= 0.8:
                    effect_category = 'Large'
                elif abs_d >= 0.5:
                    effect_category = 'Medium'
                elif abs_d >= 0.2:
                    effect_category = 'Small'
                else:
                    effect_category = 'Negligible'
                
                writer.writerow([
                    comparison_name.replace('_vs_', ' vs '),
                    f"{t_stat:.4f}",
                    f"{p_val:.6f}",
                    f"{cohens_d:.4f}",
                    significant,
                    effect_category
                ])
    
    def run_extended_comparison(self):
        """í™•ì¥ëœ ë¹„êµ ì‹¤í—˜ ì „ì²´ ì‹¤í–‰"""
        
        logger.info("í™•ì¥ëœ ì „ì´í•™ìŠµ ë¹„êµ ì‹¤í—˜ ì‹œì‘!")
        logger.info("=" * 60)
        
        # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
        self.results_dir = f"extended_results_{self.experiment_id}"
        import os
        os.makedirs(self.results_dir, exist_ok=True)
        
        try:
            # 1. ì´ì „ ê²°ê³¼ ë¡œë“œ
            if not self.load_previous_results():
                logger.error("ì´ì „ ì‹¤í—˜ ê²°ê³¼ê°€ í•„ìš”í•©ë‹ˆë‹¤. ë¨¼ì € ê¸°ë³¸ ì‹¤í—˜ì„ ì™„ë£Œí•˜ì„¸ìš”.")
                return None
            
            # 2. í™˜ê²½ ìƒì„±
            env = self.create_ev_environment()
            if not env:
                return None
            
            # 3. MountainCar ì „ì´í•™ìŠµ ì‹¤í–‰
            mountaincar_model, mountaincar_info = self.train_mountaincar_transfer_model(env)
            
            if mountaincar_model:
                # 4. MountainCar ëª¨ë¸ í‰ê°€
                mountaincar_results = self.evaluate_mountaincar_model(mountaincar_model, env)
                self.results['sac_mountaincar'] = mountaincar_results
            else:
                logger.error("MountainCar ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨")
                return None
            
            # 5. 4ê°œ ëª¨ë¸ í†µê³„ ë¹„êµ
            statistical_analysis = self.perform_extended_statistical_comparison()
            
            if not statistical_analysis:
                return None
            
            # 6. ì¢…í•© ì‹œê°í™” ìƒì„± (sagemaker_test.py ìˆ˜ì¤€)
            self.create_comprehensive_visualizations(statistical_analysis)
            
            # 7. ë…¼ë¬¸ìš© í…Œì´ë¸” ìƒì„±
            self.create_publication_ready_tables(statistical_analysis)
            
            # 8. ìƒì„¸ ë³´ê³ ì„œ ìƒì„±
            report = self.generate_extended_report(statistical_analysis)
            
            # 9. ê²°ê³¼ ì €ì¥
            final_results = {
                'experiment_id': self.experiment_id,
                'models_compared': list(self.results.keys()),
                'individual_results': self.results,
                'statistical_analysis': statistical_analysis,
                'mountaincar_transfer_info': mountaincar_info,
                'visualization_files': {
                    'comprehensive_comparison': f'{self.results_dir}/comprehensive_4model_comparison.png',
                    'detailed_statistical': f'{self.results_dir}/detailed_statistical_analysis.png',
                    'transfer_learning_analysis': f'{self.results_dir}/transfer_learning_analysis.png'
                },
                'publication_files': {
                    'latex_tables': f'{self.results_dir}/publication_tables.tex',
                    'csv_tables': f'{self.results_dir}/performance_comparison.csv',
                    'markdown_report': f'extended_comparison_report_{self.experiment_id}.md'
                }
            }
            
            with open(f'{self.results_dir}/complete_results.json', 'w') as f:
                json.dump(final_results, f, indent=2, default=str)
            
            # 10. ìµœì¢… ìš”ì•½ ì¶œë ¥
            logger.info(" í™•ì¥ëœ ë¹„êµ ì‹¤í—˜ ì™„ë£Œ!")
            logger.info("\n ìµœì¢… ìˆœìœ„:")
            
            for i, (model_name, efficiency) in enumerate(statistical_analysis['ranking'], 1):
                model_display = {
                    'cruise_mode': 'í¬ë£¨ì¦ˆ ëª¨ë“œ',
                    'sac_scratch': 'SAC ìˆœìˆ˜í•™ìŠµ',
                    'sac_lunarlander': 'SAC LunarLander ì „ì´',
                    'sac_mountaincar': 'SAC MountainCar ì „ì´'
                }.get(model_name, model_name)
                
                logger.info(f"  {i}. {model_display}: {efficiency:.3f} km/kWh")
            
            # ì „ì´í•™ìŠµ ë¹„êµ ê²°ê³¼
            if ('sac_mountaincar' in statistical_analysis['model_means'] and 
                'sac_lunarlander' in statistical_analysis['model_means']):
                
                mountain_eff = statistical_analysis['model_means']['sac_mountaincar']['mean']
                lunar_eff = statistical_analysis['model_means']['sac_lunarlander']['mean']
                
                if mountain_eff > lunar_eff:
                    logger.info(f"\nğŸ† ìµœì  ì „ì´í•™ìŠµ: MountainCar ({mountain_eff:.3f} > {lunar_eff:.3f})")
                else:
                    logger.info(f"\nğŸ† ìµœì  ì „ì´í•™ìŠµ: LunarLander ({lunar_eff:.3f} > {mountain_eff:.3f})")
            
            logger.info(f"\n ìƒì„±ëœ íŒŒì¼ë“¤:")
            logger.info(f"   ì‹œê°í™”: {self.results_dir}/comprehensive_4model_comparison.png")
            logger.info(f"  ğŸ“ˆ í†µê³„ë¶„ì„: {self.results_dir}/detailed_statistical_analysis.png") 
            logger.info(f"  ğŸ”„ ì „ì´í•™ìŠµ: {self.results_dir}/transfer_learning_analysis.png")
            logger.info(f"  ğŸ“‹ LaTeX í…Œì´ë¸”: {self.results_dir}/publication_tables.tex")
            logger.info(f"  ğŸ“„ ë³´ê³ ì„œ: extended_comparison_report_{self.experiment_id}.md")
            logger.info(f"  ğŸ’¾ ì „ì²´ê²°ê³¼: {self.results_dir}/complete_results.json")
            
            return final_results
            
        except Exception as e:
            logger.error(f"í™•ì¥ëœ ë¹„êµ ì‹¤í—˜ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def generate_extended_report(self, statistical_analysis):
        """í™•ì¥ëœ ë¹„êµ ë³´ê³ ì„œ ìƒì„± (ì‹œê°í™” í¬í•¨)"""
        
        logger.info("ğŸ“‹ í™•ì¥ëœ ë¹„êµ ë³´ê³ ì„œ ìƒì„±...")
        
        report = f"""# í™•ì¥ëœ ì „ì´í•™ìŠµ ë¹„êµ ì‹¤í—˜ ë³´ê³ ì„œ
## MountainCar vs LunarLander vs ìˆœìˆ˜í•™ìŠµ vs í¬ë£¨ì¦ˆ ëª¨ë“œ

### ğŸ”¬ ì‹¤í—˜ ê°œìš”
- **ì‹¤í—˜ ID**: {self.experiment_id}
- **ë¹„êµ ëª¨ë¸**: 4ê°œ (í¬ë£¨ì¦ˆ, ìˆœìˆ˜, LunarLander ì „ì´, MountainCar ì „ì´)
- **í†µê³„ì  ê²€ì •ë ¥**: ê° ëª¨ë¸ë‹¹ 30 ì—í”¼ì†Œë“œ
- **ì‹ ë¢°ìˆ˜ì¤€**: 95%
- **ë°ì´í„°**: í†µê³„ì ìœ¼ë¡œ ê²€ì¦ëœ ì¦ê°• ë°ì´í„° (972í–‰ í›ˆë ¨, 216í–‰ í…ŒìŠ¤íŠ¸)

### ğŸ† ì„±ëŠ¥ ìˆœìœ„ (ì—ë„ˆì§€ íš¨ìœ¨ ê¸°ì¤€)
"""
        
        for i, (model_name, efficiency) in enumerate(statistical_analysis['ranking'], 1):
            model_display = {
                'cruise_mode': 'í¬ë£¨ì¦ˆ ëª¨ë“œ',
                'sac_scratch': 'SAC ìˆœìˆ˜í•™ìŠµ',
                'sac_lunarlander': 'SAC LunarLander ì „ì´',
                'sac_mountaincar': 'SAC MountainCar ì „ì´'
            }.get(model_name, model_name)
            
            # ì‹ ë¢°êµ¬ê°„ ì¶”ê°€
            if model_name in self.results and 'confidence_interval_95' in self.results[model_name]:
                ci = self.results[model_name]['confidence_interval_95']
                ci_str = f" (95% CI: [{ci[0]:.3f}, {ci[1]:.3f}])"
            else:
                ci_str = ""
            
            report += f"{i}. **{model_display}**: {efficiency:.3f} km/kWh{ci_str}\n"
        
        report += "\n###  ìƒì„¸ ì„±ëŠ¥ ê²°ê³¼\n"
        
        for model_name, data in statistical_analysis['model_means'].items():
            model_display = {
                'cruise_mode': 'í¬ë£¨ì¦ˆ ëª¨ë“œ',
                'sac_scratch': 'SAC ìˆœìˆ˜í•™ìŠµ', 
                'sac_lunarlander': 'SAC LunarLander ì „ì´',
                'sac_mountaincar': 'SAC MountainCar ì „ì´'
            }.get(model_name, model_name)
            
            report += f"- **{model_display}**: {data['mean']:.3f} Â± {data['std']:.3f} km/kWh (n={data['n']})\n"
        
        report += "\n### í¬ë£¨ì¦ˆ ëª¨ë“œ ëŒ€ë¹„ ê°œì„ ìœ¨\n"
        
        for model, improvement in statistical_analysis['improvement_rates'].items():
            model_display = {
                'sac_scratch': 'SAC ìˆœìˆ˜í•™ìŠµ',
                'sac_lunarlander': 'SAC LunarLander ì „ì´', 
                'sac_mountaincar': 'SAC MountainCar ì „ì´'
            }.get(model, model)
            
            status = ' ëª©í‘œ ë‹¬ì„±' if improvement >= 20 else 'ëª©í‘œ ë¯¸ë‹¬ì„±'
            report += f"- **{model_display}**: {improvement:+.1f}% ({status})\n"
        
        report += "\n### ğŸ“ˆ ì „ì´í•™ìŠµ ëª¨ë¸ ë¹„êµ\n"
        
        # LunarLander vs MountainCar ì§ì ‘ ë¹„êµ
        mountain_vs_lunar_key = None
        for key in statistical_analysis['pairwise_comparisons']:
            if 'mountaincar' in key and 'lunarlander' in key:
                mountain_vs_lunar_key = key
                break
        
        if mountain_vs_lunar_key:
            comparison = statistical_analysis['pairwise_comparisons'][mountain_vs_lunar_key]
            
            winner = "MountainCar" if comparison['mean_difference'] < 0 else "LunarLander"
            significance = "í†µê³„ì ìœ¼ë¡œ ìœ ì˜" if comparison['t_test']['significant'] else "í†µê³„ì ìœ¼ë¡œ ë¹„ìœ ì˜"
            effect_size = abs(comparison['effect_size'])
            
            if effect_size >= 0.8:
                effect_desc = "ëŒ€ íš¨ê³¼"
            elif effect_size >= 0.5:
                effect_desc = "ì¤‘ íš¨ê³¼"
            elif effect_size >= 0.2:
                effect_desc = "ì†Œ íš¨ê³¼"
            else:
                effect_desc = "ë¬´ì‹œí•  ìˆ˜ ìˆëŠ” íš¨ê³¼"
            
            report += f"- **ìµœì  ì „ì´í•™ìŠµ ëª¨ë¸**: {winner}\n"
            report += f"- **í†µê³„ì  ìœ ì˜ì„±**: {significance} (p = {comparison['t_test']['p_value']:.4f})\n"
            report += f"- **íš¨ê³¼í¬ê¸°**: {effect_desc} (Cohen's d = {comparison['effect_size']:.3f})\n"
        
        # í•™ìŠµ íš¨ìœ¨ì„± ë¶„ì„
        report += "\n### âš¡ í•™ìŠµ íš¨ìœ¨ì„± ë¶„ì„\n"
        
        if 'sac_scratch' in statistical_analysis['model_means']:
            scratch_perf = statistical_analysis['model_means']['sac_scratch']['mean']
            
            report += f"- **ìˆœìˆ˜í•™ìŠµ (100k ìŠ¤í…)**: {scratch_perf:.3f} km/kWh\n"
            
            for model_key, steps in [('sac_lunarlander', 50000), ('sac_mountaincar', 50000)]:
                if model_key in statistical_analysis['model_means']:
                    model_perf = statistical_analysis['model_means'][model_key]['mean']
                    efficiency_per_step = (model_perf / steps) * 1000
                    model_name = 'LunarLander' if 'lunar' in model_key else 'MountainCar'
                    
                    report += f"- **{model_name} ì „ì´ ({steps//1000}k ìŠ¤í…)**: {model_perf:.3f} km/kWh "
                    report += f"(íš¨ìœ¨: {efficiency_per_step:.4f}/1kìŠ¤í…)\n"
        
        report += f"\n###  í†µê³„ì  ê²€ì¦ ê²°ê³¼\n"
        
        # ANOVA ê²°ê³¼
        if statistical_analysis['anova_test']:
            anova = statistical_analysis['anova_test']
            significance = "ìœ ì˜í•¨" if anova['significant'] else "ë¹„ìœ ì˜í•¨"
            report += f"- **ì¼ì›ë°°ì¹˜ ë¶„ì‚°ë¶„ì„**: F = {anova['f_statistic']:.3f}, "
            report += f"p = {anova['p_value']:.4f} ({significance})\n"
        
        # ì£¼ìš” ìŒë³„ ë¹„êµ
        key_comparisons = [
            ('sac_mountaincar_vs_sac_lunarlander', 'MountainCar vs LunarLander'),
            ('sac_scratch_vs_cruise_mode', 'ìˆœìˆ˜í•™ìŠµ vs í¬ë£¨ì¦ˆ'),
        ]
        
        for comp_key, comp_name in key_comparisons:
            if comp_key in statistical_analysis['pairwise_comparisons']:
                comp = statistical_analysis['pairwise_comparisons'][comp_key]
                sig_status = "ìœ ì˜" if comp['t_test']['significant'] else "ë¹„ìœ ì˜"
                report += f"- **{comp_name}**: t = {comp['t_test']['statistic']:.3f}, "
                report += f"p = {comp['t_test']['p_value']:.4f} ({sig_status})\n"
        
        report += f"\n### ğŸ¨ ìƒì„±ëœ ì‹œê°í™”\n"
        report += f"- **ì¢…í•© ì„±ëŠ¥ ë¹„êµ**: `{self.results_dir}/comprehensive_4model_comparison.png`\n"
        report += f"- **ìƒì„¸ í†µê³„ ë¶„ì„**: `{self.results_dir}/detailed_statistical_analysis.png`\n"
        report += f"- **ì „ì´í•™ìŠµ ë¶„ì„**: `{self.results_dir}/transfer_learning_analysis.png`\n"
        
        report += f"\n### ğŸ“‹ ë…¼ë¬¸ìš© ìë£Œ\n"
        report += f"- **LaTeX í…Œì´ë¸”**: `{self.results_dir}/publication_tables.tex`\n"
        report += f"- **CSV ë°ì´í„°**: `{self.results_dir}/performance_comparison.csv`\n"
    
    def run_extended_comparison(self):
        """í™•ì¥ëœ ë¹„êµ ì‹¤í—˜ ì „ì²´ ì‹¤í–‰"""
        
        logger.info("í™•ì¥ëœ ì „ì´í•™ìŠµ ë¹„êµ ì‹¤í—˜ ì‹œì‘!")
        logger.info("=" * 60)
        
        try:
            # 1. ì´ì „ ê²°ê³¼ ë¡œë“œ
            if not self.load_previous_results():
                logger.error("ì´ì „ ì‹¤í—˜ ê²°ê³¼ê°€ í•„ìš”í•©ë‹ˆë‹¤. ë¨¼ì € ê¸°ë³¸ ì‹¤í—˜ì„ ì™„ë£Œí•˜ì„¸ìš”.")
                return None
            
            # 2. í™˜ê²½ ìƒì„±
            env = self.create_ev_environment()
            if not env:
                return None
            
            # 3. MountainCar ì „ì´í•™ìŠµ ì‹¤í–‰
            mountaincar_model, mountaincar_info = self.train_mountaincar_transfer_model(env)
            
            if mountaincar_model:
                # 4. MountainCar ëª¨ë¸ í‰ê°€
                mountaincar_results = self.evaluate_mountaincar_model(mountaincar_model, env)
                self.results['sac_mountaincar'] = mountaincar_results
            else:
                logger.error("MountainCar ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨")
                return None
            
            # 5. 4ê°œ ëª¨ë¸ í†µê³„ ë¹„êµ
            statistical_analysis = self.perform_extended_statistical_comparison()
            
            if not statistical_analysis:
                return None
            
            # 6. ê²°ê³¼ ì €ì¥
            final_results = {
                'experiment_id': self.experiment_id,
                'models_compared': list(self.results.keys()),
                'individual_results': self.results,
                'statistical_analysis': statistical_analysis,
                'mountaincar_transfer_info': mountaincar_info
            }
            
            with open(f'extended_comparison_results_{self.experiment_id}.json', 'w') as f:
                json.dump(final_results, f, indent=2, default=str)
            
            # 7. ë³´ê³ ì„œ ìƒì„±
            report = self.generate_extended_report(statistical_analysis)
            
            # 8. ìš”ì•½ ì¶œë ¥
            logger.info(" í™•ì¥ëœ ë¹„êµ ì‹¤í—˜ ì™„ë£Œ!")
            logger.info("\n ìµœì¢… ìˆœìœ„:")
            
            for i, (model_name, efficiency) in enumerate(statistical_analysis['ranking'], 1):
                model_display = {
                    'cruise_mode': 'í¬ë£¨ì¦ˆ ëª¨ë“œ',
                    'sac_scratch': 'SAC ìˆœìˆ˜í•™ìŠµ',
                    'sac_lunarlander': 'SAC LunarLander ì „ì´',
                    'sac_mountaincar': 'SAC MountainCar ì „ì´'
                }.get(model_name, model_name)
                
                logger.info(f"  {i}. {model_display}: {efficiency:.3f} km/kWh")
            
            return final_results
            
        except Exception as e:
            logger.error(f"í™•ì¥ëœ ë¹„êµ ì‹¤í—˜ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return None


# ì‹¤í–‰ í•¨ìˆ˜
def run_extended_transfer_comparison():
    """í™•ì¥ëœ ì „ì´í•™ìŠµ ë¹„êµ ì‹¤í–‰"""
    
    print(" í™•ì¥ëœ ì „ì´í•™ìŠµ ë¹„êµ ì‹¤í—˜ ì‹œì‘!")
    print(" ë¹„êµ ëª¨ë¸: í¬ë£¨ì¦ˆ + ìˆœìˆ˜í•™ìŠµ + LunarLander ì „ì´ + MountainCar ì „ì´")
    print("ëª©í‘œ: ìµœì ì˜ ì „ì´í•™ìŠµ ëª¨ë¸ ì‹ë³„ ë° ì¢…í•© ì„±ëŠ¥ ë¶„ì„")
    print("=" * 70)
    
    comparison = ExtendedTransferLearningComparison()
    results = comparison.run_extended_comparison()
    
    if results:
        print("\n í™•ì¥ëœ ë¹„êµ ì‹¤í—˜ ì„±ê³µ!")
        print(" 4ê°œ ëª¨ë¸ ì¢…í•© ë¶„ì„ ì™„ë£Œ")
        print(" ìµœì  ì „ì´í•™ìŠµ ëª¨ë¸ ì‹ë³„")
        print(" ë°ì´í„° í™•ë³´")
        print("\n ìƒì„±ëœ íŒŒì¼:")
        print(f"  - extended_comparison_results_{comparison.experiment_id}.json")
        print(f"  - extended_comparison_report_{comparison.experiment_id}.md")
    else:
        print("\ní™•ì¥ëœ ë¹„êµ ì‹¤í—˜ ì‹¤íŒ¨")
        print("ê¸°ë³¸ ì‹¤í—˜ ì™„ë£Œ í›„ ì¬ì‹œë„í•˜ì„¸ìš”")
    
    return results


if __name__ == "__main__":
    # ì¦‰ì‹œ ì‹¤í–‰
    results = run_extended_transfer_comparison()
